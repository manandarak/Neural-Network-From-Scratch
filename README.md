# Neural-Network-From-Scratch
This repository contains a project that demonstrates how to build a neural network from the ground up using only Python and NumPy, without relying on high-level libraries like TensorFlow or PyTorch. 
The project covers essential concepts of neural networks, including:

Implementation of Layers: Fully connected layers, activation functions (ReLU, Sigmoid, etc.), and loss functions.
Forward and Backward Propagation: Detailed implementation of forward and backward pass to update weights and biases.
Training Algorithms: Gradient descent and its variants for optimizing the neural network.
This project is designed for educational purposes, aimed at helping you understand the inner workings of neural networks by coding them from scratch.

